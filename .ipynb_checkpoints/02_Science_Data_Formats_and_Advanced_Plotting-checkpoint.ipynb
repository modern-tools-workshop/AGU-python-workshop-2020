{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Data Formats and Advanced Plotting\n",
    "\n",
    "Rebekah Esmaili (rebekah.esmaili@noaa.gov) Research Scientist, STC/JPSS\n",
    " \n",
    "---\n",
    "\n",
    "\n",
    "## Lesson 2 Objectives\n",
    "* You will learn to:\n",
    "    * Import relevant packages for scientific programming\n",
    "    * Read netCDF and GRIB2 data\n",
    "    * Creating plots and maps\n",
    "   \n",
    "---\n",
    "\n",
    "## What do I need?\n",
    "* If you are really new to Python, I recommend using the binder links to run these notebooks remotely.\n",
    "* If you have some experience, you can either install Anaconda locally on your laptop or on a remote server. There are some instructions in the main directory.\n",
    "* I _do not recommend_ using system or shared Python installations unless you are advanced!\n",
    "\n",
    "---\n",
    "\n",
    "## Importing NetCDF files\n",
    "\n",
    "NetCDF and HDF are self-describing formats, which are structured binary data files and useful for storing other big datasets. Computationally, it is faster to read in binary-based datasets than text, which needs to be parsed before being stored into a computer’s memory. Because the files are more compact, they are cheaper to store large, long-term satellite data. Furthermore, information about the data can be stored inside the file themselves.\n",
    "\n",
    "Datasets:\n",
    "* JRR-AOD_v2r3_j01_s202009152044026_e202009152045271_c202009152113150_thinned.nc: A netCDF file that contains Aerosol Optical Depth (AOD) retrieved from a Suomi NPP overpass on 2020 9 Aug.  For this workshop, unused fields were removed.\n",
    "* gfs_3_20200915_0000_000.grb2: A GRIB2 file that contains GFS analysis\n",
    "* MOP03JM-201811-L3V95.6.3_thinned.nc: The Nov 2018 CO monthly mean from the Measurement of Pollution in the Troposphere (MOPITT), which is an instrument on the Terra satellite. For this workshop, the file was converted to a netCDF4 file and unused fields were removed.\n",
    "\n",
    "Many environmental dataset names are quite long. However, the dataset name is encoded to give us information about the contents. For example:\n",
    "\n",
    "```\n",
    "JRR-AOD_v2r3_j01_s202009152044026_e202009152045271_c202009152113150.nc\n",
    "```\n",
    "You can learn several important features of the dataset without opening it:\n",
    "\n",
    "* Prefix indicates the mission (JRR, for JPSS Risk Reduction)\n",
    "* Product (Aerosol Optical Depth, or AOD), algorithm version\n",
    "* Revision number (v1r1)\n",
    "* Satellite source (j01 for JPSS-1/NOAA-20)\n",
    "* Start (s), end (e), and creation (c) time, which are each followed by the year, month, day, hour, minute, and seconds (to one decimal place). \n",
    "\n",
    "In the previous session, you learned how to import three very common packages in Python. If you are unfamiliar with packages or the specific ones below, please review the notes from last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, you need to first import the [netCDF4 package](https://unidata.github.io/netcdf4-python/netCDF4/index.html). There are other modules that can open netCDF4 files, such as [xarray](http://xarray.pydata.org/en/stable/io.html), which has the netCDF4 package as a dependency. So, it is useful to first understand the netCDF4 package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Dataset function to import the above dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname='data/JRR-AOD_v2r3_j01_s202009152044026_e202009152045271_c202009152113150_thinned.nc'\n",
    "aod_file_id = Dataset(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print the contents of the file_id variable, you will get a long list of the global attributes, variables, dimensions, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aod_file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above is worth inspecting. From the first bolded line, this file follows netCDF4 [CF-1.5 conventions](https://cfconventions.org/). Some of the information that we saw in the file name is also present: this product is the *JPSS Risk Reduction Unique Aerosol Optical Depth* (title) *Level 2* product (processing_level) and the data was collected from the *NOAA-20* (satellite_name) *VIIRS* instrument (instrument_name). The *start* (time_coverage_start) and *end* times (time_coverage_end) metadata fields are consistent with the filename. I recommend that you read netCDF file header contents, especially the first time you are working with new data. Note that you can also use tools like [Panoply](https://www.giss.nasa.gov/tools/panoply/) to inspect the contents of the netCDF file outside of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aod_file_id.variables.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, we will use the AOD500 variable, which is the primary product in this file. AOD is a unitless measure of the extinction of solar radiation by particles suspended in the atmosphere. High values of AOD can indicate the presence of dust, smoke, or another air pollutant while low values indicate a cleaner atmosphere.\n",
    "\n",
    "**A quick NumPy recap!**\n",
    "Using NumPy, we can access individual elements using an index, with zero being the first element. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_array = np.array([4, 8, 15, 16, 23, 42])\n",
    "num_array[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access all numbers using the colon (:) inside the square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_array[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can extract AOD using the *.variable* function. It's a 2-dimensional array, so the code below has two \\[:,:\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AOD_550 = aod_file_id.variables['AOD550'][:,:]\n",
    "AOD_lat = aod_file_id.variables['Latitude'][:,:]\n",
    "AOD_lon = aod_file_id.variables['Longitude'][:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the type of *AOD_500*, you can see it's a *masked array*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(AOD_550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values are masked out, so if we do statistics on the array, it will not include them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgAOD = AOD_550.mean()\n",
    "print(avgAOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise 1**: Importing netCDF files\n",
    "1. Open the file \"MOP03JM-201811-L3V95.6.3_thinned.nc\" using the netCDF4 library\n",
    "2. Print the variable names\n",
    "3. What are the dimensions?\n",
    "---\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing GRIB2 files\n",
    "\n",
    "GRIB2 files is a binary datasets that take on a table-driven code form. \"Table driven\" means that the files require external tables to decode the data type. Thus, they are not self-describing. These files follow a methodology of encoding binary data and not a distinct file type. Binary Universal Form for the Representation of meteorological data (BUFR) and GRIdded Binary (GRIB) are two common table-driven formats in Earth Sciences. \n",
    "\n",
    "American NWS models (e.g. GFS, NAM, and HRRR) and the European (e.g. ECMWF) models are stored in GRIB2. While they share the same format, there are some differences in how each organization stores its data. GRIB2 are stored as binary variables with a header describing the data stored followed by the variable values.\n",
    "\n",
    "Currently, some of the GRIB2 decoders have problems parsing the American datasets because the American models have multiple pressure dimensions (depending on the variable) while the European models have one. Still, there are ways the data can be inspected by using the pygrib and cfgrib packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pygrib package (Unidata) has an interface between Python and the GRIB-API (ECMWF). ECMWF has since ended support for the GRIB-API as the primary GRIB2 encoded and decoder and now use ecCodes. However, the package is still maintained by the developer (https://jswhit.github.io/pygrib/) and is useful for parsing NCEP weather forecast data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/gfs_3_20200915_0000_000.grb2'\n",
    "gfs_grb2 = pygrib.open(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This opens the file, but does not extract the elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gfs_grb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a *for loop* in Python. The code block below will iterate over each item in the open dataset and append (using *.append*) them to a list (*records*). Note that if you run this command again, you will read to the end of the file, so there will be no result. You will have to re-open the command and re-run the block below.\n",
    "\n",
    "You can check the size of the final list using *len(messages)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for grb in gfs_grb2:\n",
    "    records.append(str(grb))\n",
    "    \n",
    "len(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 522 individual data product definition in this file, so first let’s inspect the contents of one line to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, you can see that the colons (:) separate the sections of the product definition in this GRIB2 message. The elements are *index* (1), *variable name* and *units* (2-3), and *spatial*, *vertical*, and *temporal* definitions (4-8). There is one record for each *pressure level* and *time*. You can then extract all variables using the *.select(name=\\[variable\\])* command. Below, you select all the Temperature records (there are 46, which you can see by using the *len(temps)* command). Since it is a long list, you are only printing some of these below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = gfs_grb2.select(name='Temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to extract temperature at 85000 Pa, you can use the index (*315*) to pull that record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = gfs_grb2[315]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, using *.values* you can extract the data from the record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also extract the grid information and other import metadata for this record. To see all available information, use the *.keys()* command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates can be extracted using the *.latitude* and *.longitude*. You can additionally extract the level, units, and forecast time from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = temp.latitudes\n",
    "lon = temp.longitudes\n",
    "\n",
    "level = temp.level\n",
    "units = temp.units\n",
    "\n",
    "analysis_date = temp.analDate\n",
    "fcst_time = temp.forecastTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to import multidimensional data, you will make some plots in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting 3-dimensional Data\n",
    "\n",
    "In Example 1, you imported total CO. Below, I am reproducing this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data/MOP03JM-201811-L3V95.6.3_thinned.nc' \n",
    "mop_file_id = Dataset(fname, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that to inspect the groups and variables, you can use the visit command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mop_file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import *RetrievedCOTotalColumnDay* which is a 2-dimensional variable. You will also need latitude and longitude, which are both one dimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = mop_file_id[\"RetrievedCOTotalColumnDay\"][:,:]\n",
    "lat = mop_file_id[\"Latitude\"][:]\n",
    "lon = mop_file_id[\"Longitude\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contour plots and mesh plots are two useful ways of looking at 3-dimensional data. Both plots require the x, y, and z coordinates to have the same 2-dimensional grid. However, lat and lon are 1-dimensional. You can use *np.meshgrid()* to project the 1-dimensional x and y coordinates into two dimensions.\n",
    "\n",
    "This function is a little confusing at first, so I'll show a simple example. Suppose you have to simple arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x = [1,2]\n",
    "tmp_y = [3,4,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*tmp_x* has two elements and *tmp_y* has three. If you create a mesh of the two variables, there will be two variables, both with 3 rows and 2 columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.meshgrid(tmp_x, tmp_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the example, below is the meshgrid of the 1-dimensional latitude and longitude coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_co, Y_co = np.meshgrid(lon, lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before plotting, you need to check if all the dimensions match. However, after comparing the shape of co to X_co, you can see that the dimensions are flipped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.shape, X_co.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the two arrays match, you can use the *.transpose()* function to switch the x and y coordinates in co."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = co.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the data, co has many -9999. values, which are likely missing values. Below, you extract the missing value and save it to a value called *missing*. Then, you replace all missing values with *np.nan* so that they are not plot included in the plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last session, you learned how to use *plt.subplot()* to generate the empty figure (*fig*)and axis (*ax*). \n",
    "\n",
    "One line 2, you call *ax.contourf* and input the X_co, Y_co, and transposed co variables. co acts as a color value, which becomes the third dimension of the plot. You then store this object into a variable *co_plot* so that you can pass it into *ax.colorbar* in order to map the colors to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contourf\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "co_plot = ax.contourf(X_co, Y_co, co)\n",
    "fig.colorbar(co_plot, orientation='horizontal', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above, you can see that there are regions where there are higher levels of CO (in molecules/cm2). The data are clustered together and have global coverage, so a contour plot is a relevant choice in this scenario.\n",
    "\n",
    "Like contour plots, mesh plots are also 2-dimensional plots that display 3-dimensions of information using x, y, coordinates and z for a color scale. However, mesh plots do not perform any smoothing and display data as-is on a regular grid. However, since many satellite datasets are swath-based, irregularly spaced data needs to be re-gridded in order to display it as a mesh grid. In the code block below, let’s compare how the MOPITT data looks using pcolormesh command with the previous example using contour. The code below has no other changes to the plot other than the call to the plot type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pcolormesh\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "co_plot = ax.pcolormesh(X_co, Y_co, co, shading='auto')\n",
    "fig.colorbar(co_plot, orientation='horizontal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that there is more structure in the mesh plot than the filled contour. This is useful if you wish to examine fine structure and patterns.\n",
    "\n",
    "---\n",
    "**Exercise 2**: Plot 3-dimensional data\n",
    "\n",
    "Plot *AOD_lat*, *AOD_lon*, and *AOD_500* (which we imported from the \"JRR-AOD_v2r3_j01_...\" netCDF file as:\n",
    "\n",
    "1. Check the dimensions for all variables using *.shape*.\n",
    "2. Do you need to generate a meshgrid with *np.meshgrid()*?\n",
    "3. Create a contour plot\n",
    "\n",
    "---\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AOD_lat.shape, AOD_lon.shape\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "co_plot = ax.contourf(AOD_lon, AOD_lat, AOD_550)\n",
    "fig.colorbar(co_plot, orientation='horizontal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Maps to datasets\n",
    "\n",
    "The package Cartopy add mapping functionality to Matplotlib. Cartopy provides an interface to obtain continent, country, and feature details to overlay onto your plot. Furthermore, Cartopy also enables you to convert your data from one map projection to another, which requires a cartesian coordinate system to the map coordinates. Matplotlib natively supports the six mathematical and map projections (Aitoff, Hammer, Lambert, Mollweide, polar, and rectilinear) and combined with Cartopy, data can be transformed to a total of 33 possible projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartopy import crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs_temp = temp.values\n",
    "gfs_x, gfs_y = np.meshgrid(lon, lat)\n",
    "\n",
    "gfs_x.shape, gfs_y.shape, gfs_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs_temp[0:180,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,5])\n",
    "ax = plt.subplot(projection=ccrs.PlateCarree())\n",
    "\n",
    "ax.contourf(gfs_x, gfs_y, gfs_temp[0:180,:])\n",
    "\n",
    "ax.coastlines('50m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, you can switch from Plate Carrée to Orthographic. You must define the projection twice, once in the *projection=* keyword and again in the *transform=*. In the *plt.subplot* line, you must define the to coordinates (*ccrs.Orthographic*), which is how you want to axes to show the data.  In the ax.scatter line, you use the transform keyword argument in scatter to define the from coordinates (Plate Carrée), which are the coordinates that the data formatted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,5])\n",
    "ax = plt.subplot(projection=ccrs.Orthographic(-90, 0))\n",
    "\n",
    "ax.contourf(gfs_x, gfs_y, gfs_temp[0:180,:], transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.coastlines('50m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise 3** Adding maps to plots\n",
    "\n",
    "Using *AOD_lat*, *AOD_lon*, and *AOD_550* (which we imported from the \"JRR-AOD_v2r3_j01_...\" netCDF file)\n",
    "\n",
    "1. Create a *pcolormesh* plot\n",
    "2. Add the coastlines to a standard Plate Caree plot using *projection=* option.\n",
    "\n",
    "---\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,5])\n",
    "ax = plt.subplot(projection=ccrs.PlateCarree())\n",
    "\n",
    "ax.contourf(AOD_lon, AOD_lat, AOD_550)\n",
    "\n",
    "ax.coastlines('50m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripting with Python\n",
    "\n",
    "### Creating scripts from Jupyter Notebooks\n",
    "One of the simplest ways to create a script is to convert an existing Jupyter notebook. As an example, we will created a notebook named script_example that only contains one line of code: print(“Hello Earth”). You can convert any Jupyter Notebook to a script by going to File → Download as → Python (.py):\n",
    " \n",
    "\n",
    "This will download a new file (script_example.py) to your computer. If you open the file using your text editor, you will see:\n",
    "\n",
    "```\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "print(\"Hello Earth\")\n",
    "```\n",
    "\n",
    "You will notice that the script contains the line numbers (*ln\\[1\\]*), which in my opinion is unnecessary and should be removed from your script. Beginners, you can delete this extra formatting from your file.\n",
    "\n",
    "### Running Python scripts from the command line\n",
    "\n",
    "Now you are finished editing the code and you probably want to run it. There are two ways you can run Python scripts:\n",
    "\n",
    "1. Using the command line interpreter\n",
    "2. Using iPython\n",
    "\n",
    "iPython is an interactive command line that allows you to run code in chunks. In fact, Jupyter Notebook is built using iPython, which explains the similarity in behavior.\n",
    " \n",
    "* Windows: I suggest using the Anaconda Prompt which you can access from the start menu or using Anaconda Navigator. \n",
    "* MacOs/Linux: open the Terminal app. \n",
    "\n",
    "Once the command line is open, you start in a default location. For example, if you are using Windows and launch the Anaconda Prompt you will see:\n",
    "\n",
    "```\n",
    "(base) C:\\Users\\rebekah>\n",
    "```\n",
    "\n",
    "Now, navigate to where our script is. To do this, you will change directories using the cd command. For example, if your code is stored in C:\\Documents\\Python, you can type:\n",
    "\n",
    "```\n",
    "\n",
    "cd C:\\Documents\\Python\n",
    "```\n",
    "\n",
    "The command line will now be updated showing:\n",
    "\n",
    "```\n",
    "(base) C:\\Documents\\Python>\n",
    "```\n",
    "\n",
    "Now that you are in the right place, you can call the Python interpreter, which to convert your code into a format that your computer can understand and executes the command. If you installed Anaconda, this includes a Python 3 interpreter (*python3*). So, to run the script, type:\n",
    "\n",
    "```\n",
    "python3 hello_world.py\n",
    "```\n",
    "\n",
    "If successful, “Hello Earth” should print to your screen.\n",
    "\n",
    "A second method is to use iPython, which allows you to open Python in interactive mode. Unlike the command line method, iPython will let you run code line-by-line. So, like Jupyter Notebook, you have the option to copy and paste you code from the text editor in chunks into the iPython window. You can also call the entire script inside iPython. This is done by starting iPython and using the command %run \\[script name\\].py. Below is a capture from my terminal:\n",
    "\n",
    "```\n",
    "Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
    "Type 'copyright', 'credits' or 'license' for more information\n",
    "IPython 7.12.0 -- An enhanced Interactive Python. Type '?' for help.\n",
    "\n",
    "In [1]: %run script_example.ipynb\n",
    "Hello Earth\n",
    "```\n",
    "\n",
    "One advantage of using iPython is that after the script finishes running, variables that were generated in the script are still in memory. Then, you can print or operate on the variables to either debug or to develop your code further. \n",
    "\n",
    "You may have noted two differences in workflow for write code in scripts versus notebooks, (1) that code cannot be inline and (2) the program must run fully to the end.\n",
    "\n",
    "\n",
    "### Handling output when scripting\n",
    "\n",
    "In the previous example, you printed text to the screen but Python’s capable of saving figures and data. To save plots, replace *plt.show()* with the *plt.savefig()* command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to directly display your graphics using the X11 protocol (by default in Linux) with XQuartz (Mac) or PuTTy (Windows). \n",
    "\n",
    "I typically discourage this because satellite imagery tends to be very large and thus slow to display remotely. From my experience, it is usually faster to write an image to a file and then view the plot after it is fully rendered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "You learned:\n",
    "\n",
    "* Very basic built-in Python functions and operations\n",
    "* How to import scientific data formats, like netCDF and GRIB2\n",
    "* Worked with arrays and lists\n",
    "* How to create a simple plot\n",
    "* Some basic scripting with Python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
